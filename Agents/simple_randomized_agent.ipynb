{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intelligent Agents: Vacuum-cleaner World\n",
    "\n",
    "Implement a simulator environment for a vacuum-cleaner world and a set of intelligent agents.\n",
    "\n",
    "## PEAS description\n",
    "\n",
    "__Performance Measure:__ Each action costs 1. The performance is measured as the sum of cost to clean the whole environment.\n",
    "\n",
    "__Environment:__ A room with $n \\times n$ squares where $n = 5$. Dirt is randomly placed on each square with probability $p = 0.2$.\n",
    "\n",
    "__Actuators:__ The agent can `clean` the current square or move to an ajacent square by going `north`, `east`, `west`, or `south`.\n",
    "\n",
    "__Sensors:__ Four bumper sensors, one for north`, `east`, `west`, and `south`; a dirt sensor.  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The easiest implementation is for the environment to hold an array to represent the squares and call the agent function in a loop till all squares are clean. The agent function can look in Python like this\n",
    "\n",
    "## Define a simple randomized agent\n",
    "\n",
    "The agent function gets sensor information as the arguments.\n",
    "\n",
    "* A dictonary with boolean entries for the for bumper sensors `north`, `east`, `west`, `south`; not specified bumpers are assumed to be `False`. E.g., if the agent is on the north-west corner, `bumpers` gets `{\"north\" : True, \"east\" : True}` or if the agent is not close to a border then it gets `{}`.\n",
    "* The dirt sensor prodices a boolean.\n",
    "\n",
    "The agent reurns the action as a string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'north'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import random\n",
    "\n",
    "actions = [\"north\", \"east\", \"west\", \"south\", \"suck\"]\n",
    "\n",
    "\n",
    "def simple_randomized_agent(bumpers, dirty):\n",
    "    return random.choice(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_randomized_agent({\"north\" : True}, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the agent 10 times steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "south\n",
      "west\n",
      "east\n",
      "west\n",
      "east\n",
      "suck\n",
      "suck\n",
      "west\n",
      "north\n",
      "north\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(simple_randomized_agent({}, True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your environment simulator needs to create squares, make some dirty, and provde the agent function with the sensor inpits. The environment needs to track the agents actions till all dirty squares are clean and count the number of actions it takes the agent to complete the task.\n",
    "\n",
    "__Implement:__\n",
    "\n",
    "* A simple reflex agent that randomly walks around but reachts to the bumper sensor by not bumping into the wall and to dirt with sucking.\n",
    "* A model-based reflex agent that keeps track of the location and remembers where it has cleaned.\n",
    "\n",
    "Complare the agents using simulations using different size environments. E.g., $5 \\times 5$, $10 \\times 10$ and\n",
    "$100 \\times 100$. Use at least 100 random runs for each. You may need to implement a timeout if an agent does not complete the task in, e.g., 1 million steps.\n",
    "\n",
    "__Bonus:__\n",
    "\n",
    "Change the environment, so each square has a fixed probability to get dirty again. Give this information to the agent (as a 2-dimentional array of probabilities). Cleaning one dirty square produces the utility of 1. Implement a utility-based agent that maximizes the expected utility over a time horizon of 10000 time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
